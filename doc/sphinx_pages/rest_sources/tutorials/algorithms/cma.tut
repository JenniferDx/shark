Covariance Matrix Adaptation Evolution Strategy
===============================================

The covariance matrix adaptation evolution strategy (CMA-ES) is one of
the most powerful evolutionary algorithms for single-objective
real-valued optimization. The algorithm relies on normally distributed
mutative steps to explore the search space while adjusting its
mutation distribution to make successful steps from the recent past
more likely in the future. In addition, a separate step-size is
maintained and adapted.

In Shark, we provide a reference implementation of the algorithm (see
:doxy:`CMA.h`) that is integrated with the optimizer class
hierarchy. Please note that our implementation is based on the
description given in [Hansen2004]_.

This tutorial illustrates applying the CMA-ES to the :doxy:`Sphere`
benchmark function. Please note that the methods presented here apply
to all single-objective optimizers available in the Shark
library. That is, applying an optimizer to an objective function
requires the following steps:

* Instantiate and configure the objective function.
* Instantiate the optimizer.
* Configure the optimizer instance for the objective function instance.
* Execute the optimizer until a termination criterion is fulfilled.

Thee code for this tutorial can be found in 
:download:`CMASimple.cpp <../../../../../examples/EA/SOO/CMASimple.cpp>`.
First of all, the following header files are required::

..sharkcode<EA/SOO/CMASimple.tpp,includes>

Next, an instance of the objective function is created and configured
for a two-dimensional search space::

..sharkcode<EA/SOO/CMASimple.tpp,problem>

Thereafter, the optimizer is instantiated and initialized for the objective function instance::

..sharkcode<EA/SOO/CMASimple.tpp,optimizer>

Here, we delegate the setup of constants and initialization of the
algorithm's state completely to the implementation.  It tries to find a suitable estimate for the internal parameters,
namely mu and lambda and also sets a default step size. Nevertheless, the
class :doxy:`CMA` offers an additional init-method that allows for
fine grained control over the initial algorithm configuration. 
Afterwards we can use the CMA to find a solution which is good enough and print the optimization path::

..sharkcode<EA/SOO/CMASimple.tpp,train>

In general, the result of an iteration of a single-objective optimizer
is a tuple consisting of the best known search point and its
associated fitness.

Covariance Matrix Adaptation in Detail
--------------------------------------

In the example presented before, a very rough overview of the CMA, its
implementation in Shark and its application to an example problem has
been presented. We now dive into the algorithm in even more
detail. For the reaminder of the section, we assume the reader to be
familiar with the first part of the tutorial. Moreover, we assume an
audience that is familiar with the CMA-ES. 

First, we introduce the Probe framework, a unique part of Shark's core
architecture that allows for exporting values from within an algorithm
without the need to adjust its interface. It features a close-to-zero
overhead for reporting values to the outside world and an API that
delegates bookkeeping and setup tasks to sophisticated background
elements. In general, a probe is uniquely identified by its name and
the context it is living in. In the default implementation, a context
is assembled from a class-specific name (i.e., its type id) and a
class-instance specific UUID. A probe instance is observable for value
changes where the observer pattern is implemented in terms of
boost::signals.

The CMA implementation of the Shark library maintains five probes:

  =====================  ===========================================================================
    Probe name                    Probe description
  =====================  ===========================================================================
    Sigma                Reports the step size sigma.
    PopulationMean       Reports the current population mean.
    WeigthVector         Reports the current weight vector for weighted recombination.
    EvolutionPathC       Reports the evolution path for covariance matrix update purposes.
    EvolutionPathSigma	 Reports the evolution path for step size update purposes.
    CovarianceMatrix     Reports the current covariance matrix.
  =====================  ===========================================================================

Please note that the weighting vector stays constant. It is
initialized once for the selected recombination type according to the following formulas:

 =========================== ===============================================================================================================
  Recombination type                     Weigth calculation
 =========================== ===============================================================================================================
    CMA::EQUAL         		:math:`\forall 1 \leq i \leq \mu: w_i = 1/\mu`
    CMA::LINEAR        		:math:`\forall 1 \leq i \leq \mu: w'_i = \mu-i, w_i=\frac{w'_i}{\sum_{i=1}^{\mu} w'_i}`
    CMA::SUPERLINEAR   		:math:`\forall 1 \leq i \leq \mu: w'_i = \ln( \mu + 1. ) - \ln( 1. + i ), w_i=\frac{w'_i}{\sum_{i=1}^{\mu} w'_i}`
 =========================== ===============================================================================================================

Empirically Analyzing the CMA-ES
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In this section, usage of the probe framework for empirical algorithm analysis is illustrated.
The code for this part of the tutorial can be found in 
:download:`CMAProbes.cpp <../../../../../examples/EA/SOO/CMAProbes.cpp>`.
First of all, the following header files are required: ::

..sharkcode<EA/SOO/CMAProbes.cpp,includes>

In this tutorial, results are written to two comma-separated values files, i.e.:

  * results.txt: Contains the numerical results of executing the CMA.
  * plot.txt: Contains gnuplot plotting commands for subsequent visualization purposes.

Moreover, we consider the Himmelblau fitness function (see http://en.wikipedia.org/wiki/Himmelblau%27s_function ).
As before, we start over with instantiating und configuring both the optimizer and the fitness function: ::

..sharkcode<EA/SOO/CMAProbes.cpp,init>

Next, we look up the probes provided by the CMA instance and connect it to a value store: ::

..sharkcode<EA/SOO/CMAProbes.cpp,probes>

We only consider the probes reporting the population mean, the step
size sigma and the current covariance matrix in this tutorial. The
value store is a helper class that reacts to value changes, extracts
the correct data type and stores it for later use. Please see the file
:doxy:`CMAProbes.cpp` for further details.

Next, we iterate the CMA instance until a solution of sufficient quality is found.
After every iteration of the algorithm, we extract the values of
interest from the value store and write them in an appropriate format
to the result files: ::

..sharkcode<EA/SOO/CMAProbes.cpp,train>

Please note that the current solution is not equal to the population
mean reported by the CMA instance as the population mean is the result
of the recombination operator applied to the respective parent
population.

Finally, we present the results of the example in the following plot:

.. image:: ../images/cma_himmelblau.svg
   :width: 700px
   :height: 500px
   :align: center

References 
^^^^^^^^^^

.. [Hansen2004] N. Hansen and S. Kern. Evaluating the CMA Evolution Strategy on Multimodal Test Functions. In Eighth International Conference on Parallel Problem Solving from Nature PPSN VIII, Proceedings, pp. 282-291, Berlin: Springer, 2004.
